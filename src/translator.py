"""
Translator - NLLB-200ì„ ì‚¬ìš©í•œ ì¼ë³¸ì–´-í•œêµ­ì–´ ë²ˆì—­
Facebookì˜ NLLB-200 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë³´ì¡´í•˜ë©° ë²ˆì—­
"""

import torch
from pathlib import Path
from typing import List, Dict, Optional
import warnings

# Transformers ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


class Translator:
    """NLLB-200ì„ ì‚¬ìš©í•œ ì¼ë³¸ì–´-í•œêµ­ì–´ ë²ˆì—­"""

    def __init__(
        self,
        model_name: str = "facebook/nllb-200-1.3B",
        device: Optional[str] = None,
        source_lang: str = "jpn_Jpan",
        target_lang: str = "kor_Hang"
    ):
        """
        Args:
            model_name: NLLB ëª¨ë¸ ì´ë¦„
                - facebook/nllb-200-distilled-600M (ì‘ê³  ë¹ ë¦„)
                - facebook/nllb-200-1.3B (ì¤‘ê°„, ê¶Œì¥)
                - facebook/nllb-200-3.3B (í¬ê³  ì •í™•)
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™ê°ì§€)
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (jpn_Jpan: ì¼ë³¸ì–´)
            target_lang: ëª©í‘œ ì–¸ì–´ ì½”ë“œ (kor_Hang: í•œêµ­ì–´)
        """
        self.model_name = model_name
        self.source_lang = source_lang
        self.target_lang = target_lang

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"ğŸ”§ Translator ì´ˆê¸°í™”:")
        print(f"   - ëª¨ë¸: {model_name}")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ë²ˆì—­: {source_lang} â†’ {target_lang}")

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.model = None
        self.tokenizer = None
        self._load_model()

    def _load_model(self):
        """NLLB ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

            print(f"ğŸ“¥ NLLB ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ë‹¤ìš´ë¡œë“œë¨)")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                src_lang=self.source_lang
            )

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name
            ).to(self.device)

            # ì–¸ì–´ ì½”ë“œë¥¼ í† í° IDë¡œ ë³€í™˜ (NLLB í† í¬ë‚˜ì´ì € í˜¸í™˜)
            self.target_lang_id = self.tokenizer.convert_tokens_to_ids(self.target_lang)

            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
            if self.device == "cuda":
                print(f"ğŸ® GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

        except ImportError:
            raise ImportError(
                "transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "ì„¤ì¹˜: pip install transformers sentencepiece"
            )
        except Exception as e:
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def translate(
        self,
        text: str,
        max_length: int = 512,
        num_beams: int = 5,
        verbose: bool = False
    ) -> str:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­

        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ì¶œë ¥ ê¸¸ì´
            num_beams: beam search í¬ê¸° (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€

        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸

        Raises:
            RuntimeError: ë²ˆì—­ ì‹¤íŒ¨ ì‹œ
        """
        if not text or not text.strip():
            return ""

        try:
            # í† í¬ë‚˜ì´ì¦ˆ
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(self.device)

            # ë²ˆì—­ ìƒì„±
            translated_tokens = self.model.generate(
                **inputs,
                forced_bos_token_id=self.target_lang_id,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True
            )

            # ë””ì½”ë”©
            translated_text = self.tokenizer.batch_decode(
                translated_tokens,
                skip_special_tokens=True
            )[0]

            if verbose:
                print(f"ì›ë¬¸: {text}")
                print(f"ë²ˆì—­: {translated_text}")

            return translated_text

        except Exception as e:
            raise RuntimeError(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")

    def translate_segments(
        self,
        segments: List[Dict],
        verbose: bool = True,
        batch_size: int = 8
    ) -> List[Dict]:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë²ˆì—­

        Args:
            segments: Transcriberì—ì„œ ìƒì„±ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "start": 0.0,
                        "end": 2.5,
                        "text": "ã“ã‚“ã«ã¡ã¯"
                    },
                    ...
                ]
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)

        Returns:
            ë²ˆì—­ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "start": 0.0,
                    "end": 2.5,
                    "text": "ì•ˆë…•í•˜ì„¸ìš”",
                    "original": "ã“ã‚“ã«ã¡ã¯"
                },
                ...
            ]

        Raises:
            RuntimeError: ë²ˆì—­ ì‹¤íŒ¨ ì‹œ
        """
        if not segments:
            return []

        if verbose:
            print(f"\nğŸŒ ë²ˆì—­ ì‹œì‘: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")

        try:
            translated_segments = []
            total = len(segments)

            # ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, total, batch_size):
                batch = segments[i:i + batch_size]

                if verbose:
                    print(f"   - ì§„í–‰: {i + 1}-{min(i + batch_size, total)}/{total}")

                # ë°°ì¹˜ ë²ˆì—­
                texts = [seg["text"] for seg in batch]

                # í† í¬ë‚˜ì´ì¦ˆ
                inputs = self.tokenizer(
                    texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)

                # ë²ˆì—­ ìƒì„±
                translated_tokens = self.model.generate(
                    **inputs,
                    forced_bos_token_id=self.target_lang_id,
                    max_length=512,
                    num_beams=5,
                    early_stopping=True
                )

                # ë””ì½”ë”©
                translated_texts = self.tokenizer.batch_decode(
                    translated_tokens,
                    skip_special_tokens=True
                )

                # ê²°ê³¼ ì¡°í•©
                for seg, translated in zip(batch, translated_texts):
                    translated_segments.append({
                        "start": seg["start"],
                        "end": seg["end"],
                        "text": translated.strip(),
                        "original": seg["text"]
                    })

            if verbose:
                print(f"\nâœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

                # GPU ë©”ëª¨ë¦¬ ì •ë³´
                if self.device == "cuda":
                    print(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

                # ìƒ˜í”Œ ì¶œë ¥
                if translated_segments:
                    print(f"\nğŸ“ ìƒ˜í”Œ:")
                    for seg in translated_segments[:3]:
                        print(f"   [{seg['start']:.1f}s - {seg['end']:.1f}s]")
                        print(f"   ì›ë¬¸: {seg['original']}")
                        print(f"   ë²ˆì—­: {seg['text']}\n")

            return translated_segments

        except Exception as e:
            raise RuntimeError(f"ì„¸ê·¸ë¨¼íŠ¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")

    def translate_with_fallback(
        self,
        segments: List[Dict],
        verbose: bool = True,
        batch_size: int = 8
    ) -> List[Dict]:
        """
        ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€í•˜ëŠ” ì•ˆì „í•œ ë²ˆì—­

        Args:
            segments: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°

        Returns:
            ë²ˆì—­ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€)
        """
        try:
            return self.translate_segments(segments, verbose=verbose, batch_size=batch_size)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ìœ ì§€: {e}")

            # ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return [{
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"],
                "original": seg["text"]
            } for seg in segments]

    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "source_lang": self.source_lang,
            "target_lang": self.target_lang,
            "cuda_available": torch.cuda.is_available(),
            "gpu_memory_gb": torch.cuda.memory_allocated() / 1024**3 if self.device == "cuda" else 0
        }

    def clear_cache(self):
        """ìºì‹œì™€ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        import gc
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì‚­ì œ
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        gc.collect()
        
        # PyTorch CUDA ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        
        print("âœ… ìºì‹œ ë° GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


# í¸ì˜ í•¨ìˆ˜
def translate_japanese_to_korean(
    segments: List[Dict],
    model_name: str = "facebook/nllb-200-1.3B",
    verbose: bool = True
) -> List[Dict]:
    """
    ê°„ë‹¨í•œ ì¼ë³¸ì–´-í•œêµ­ì–´ ë²ˆì—­ í•¨ìˆ˜

    Args:
        segments: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        model_name: NLLB ëª¨ë¸ ì´ë¦„
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥

    Returns:
        ë²ˆì—­ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    translator = Translator(model_name=model_name)
    return translator.translate_segments(segments, verbose=verbose)

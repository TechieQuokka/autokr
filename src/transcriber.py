"""
Transcriber - Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜
OpenAI Whisper Large ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì¼ë³¸ì–´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
"""

import torch
from pathlib import Path
from typing import List, Dict, Optional
import warnings

# Whisper ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


class Transcriber:
    """Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜"""

    def __init__(
        self,
        model_name: str = "large-v3",
        device: Optional[str] = None,
        language: str = "ja"
    ):
        """
        Args:
            model_name: Whisper ëª¨ë¸ ì´ë¦„ (tiny, base, small, medium, large, large-v2, large-v3)
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™ê°ì§€)
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ja: ì¼ë³¸ì–´, en: ì˜ì–´, ko: í•œêµ­ì–´)
        """
        self.model_name = model_name
        self.language = language

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"ğŸ”§ Transcriber ì´ˆê¸°í™”:")
        print(f"   - ëª¨ë¸: {model_name}")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ì–¸ì–´: {language}")

        # Whisper ëª¨ë¸ ë¡œë“œ
        self.model = None
        self._load_model()

    def _load_model(self):
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        try:
            import whisper

            print(f"ğŸ“¥ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ë‹¤ìš´ë¡œë“œë¨)")
            self.model = whisper.load_model(
                self.model_name,
                device=self.device
            )
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
            if self.device == "cuda":
                print(f"ğŸ® GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

        except ImportError:
            raise ImportError(
                "openai-whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "ì„¤ì¹˜: pip install openai-whisper"
            )
        except Exception as e:
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def transcribe(
        self,
        audio_path: str,
        verbose: bool = True,
        temperature: float = 0.0,
        beam_size: int = 5,
        best_of: int = 5
    ) -> List[Dict]:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
            temperature: ìƒ˜í”Œë§ temperature (0.0 = greedy, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘)
            beam_size: beam search í¬ê¸° (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
            best_of: ì—¬ëŸ¬ í›„ë³´ ì¤‘ ì„ íƒ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)

        Returns:
            íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "start": 0.0,    # ì‹œì‘ ì‹œê°„(ì´ˆ)
                    "end": 2.5,      # ì¢…ë£Œ ì‹œê°„(ì´ˆ)
                    "text": "ã“ã‚“ã«ã¡ã¯"  # í…ìŠ¤íŠ¸
                },
                ...
            ]

        Raises:
            FileNotFoundError: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
            RuntimeError: ë³€í™˜ ì‹¤íŒ¨ ì‹œ
        """
        audio_path = Path(audio_path)
        if not audio_path.exists():
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")

        if verbose:
            print(f"\nğŸ¤ ìŒì„± ë³€í™˜ ì‹œì‘: {audio_path.name}")
            print(f"   - ì–¸ì–´: {self.language}")
            print(f"   - Beam size: {beam_size}")
            print(f"   - Temperature: {temperature}")

        try:
            # Whisper ì‹¤í–‰
            result = self.model.transcribe(
                str(audio_path),
                language=self.language,
                verbose=verbose,
                temperature=temperature,
                beam_size=beam_size,
                best_of=best_of,
                fp16=(self.device == "cuda")  # GPUë©´ FP16 ì‚¬ìš©
            )

            # ê²°ê³¼ í¬ë§· ë³€í™˜
            segments = []
            for segment in result["segments"]:
                segments.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip()
                })

            if verbose:
                print(f"\nâœ… ë³€í™˜ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
                print(f"   - ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])} ë¬¸ì")

                # GPU ë©”ëª¨ë¦¬ ì •ë³´
                if self.device == "cuda":
                    print(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

            return segments

        except Exception as e:
            raise RuntimeError(f"ìŒì„± ë³€í™˜ ì‹¤íŒ¨: {e}")

    def transcribe_with_full_text(
        self,
        audio_path: str,
        verbose: bool = True
    ) -> Dict:
        """
        ìŒì„± ë³€í™˜ + ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜

        Args:
            audio_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€

        Returns:
            {
                "segments": [...],  # ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
                "text": "...",      # ì „ì²´ í…ìŠ¤íŠ¸
                "language": "ja"    # ê°ì§€ëœ ì–¸ì–´
            }
        """
        segments = self.transcribe(audio_path, verbose=verbose)

        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        full_text = " ".join([seg["text"] for seg in segments])

        return {
            "segments": segments,
            "text": full_text,
            "language": self.language
        }

    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "language": self.language,
            "cuda_available": torch.cuda.is_available(),
            "gpu_memory_gb": torch.cuda.memory_allocated() / 1024**3 if self.device == "cuda" else 0
        }

    def clear_cache(self):
        """ìºì‹œì™€ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        import gc
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì‚­ì œ
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        gc.collect()
        
        # PyTorch CUDA ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        
        print("âœ… ìºì‹œ ë° GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


# í¸ì˜ í•¨ìˆ˜
def transcribe_audio(
    audio_path: str,
    model_name: str = "large-v3",
    language: str = "ja",
    verbose: bool = True
) -> List[Dict]:
    """
    ê°„ë‹¨í•œ ìŒì„± ë³€í™˜ í•¨ìˆ˜

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name: Whisper ëª¨ë¸ ì´ë¦„
        language: ì–¸ì–´ ì½”ë“œ
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥

    Returns:
        íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    transcriber = Transcriber(model_name=model_name, language=language)
    return transcriber.transcribe(audio_path, verbose=verbose)
